# LLM 配置示例（econ.simulator）
# 复制本文件为 config/llm.env 并填写真实值后再启动应用。

# Provider 选择（默认使用 OpenAI 兼容的 provider）
ECON_SIM_LLM_PROVIDER=openai

# ----------------------------
# 主要运行时配置（三项）
# ----------------------------
# 推理终点（可选）—— 用于自托管或供应商的 OpenAI-compatible base URL。
# 如果使用官方 OpenAI，可留空或使用默认值 `https://api.openai.com/v1`。
# 例子：LLM_API_ENDPOINT=https://api.openai.com/v1
LLM_API_ENDPOINT=https://api.openai.com/v1

# API Key（必需）—— 用于认证的 Key
# 例子：LLM_API_KEY=sk-xxxx
LLM_API_KEY=sk-REPLACE_ME

# 默认模型（可选）—— 当调用方未传入 model 时使用
# 例子：LLM_DEFAULT_MODEL=gpt-3.5-turbo
LLM_DEFAULT_MODEL=gpt-3.5-turbo

# 兼容回退：如果你仍在使用旧的环境变量名，代码也会检查以下变量：
# OPENAI_API_KEY, OPENAI_API_BASE, OPENAI_MODEL


# ----------------------------
# 配额与限制（可选，保留项目中的默认行为）
# ----------------------------
# 单次脚本允许的最大 LLM 调用次数（默认 1）
ECON_SIM_LLM_MAX_CALLS_PER_SCRIPT=1

# 输入 token 的近似上限（prompt 长度检查，代码使用 1 token ≈ 4 字符的粗略估算）
# 默认 1024
ECON_SIM_LLM_MAX_INPUT_TOKENS=1024

# 单次调用允许生成的最大输出 token（provider 的 max_tokens）
# 默认 512
ECON_SIM_LLM_MAX_TOKENS_PER_CALL=512


# 说明：
# - 请将包含真实密钥的 env 文件从版本控制中排除（不要提交带有密钥的文件）。
# - 本示例保留了与旧 OPENAI_* 变量的兼容性，但推荐使用 LLM_API_* 和 LLM_DEFAULT_MODEL
#   来保持与仓库中新实现的一致性。
